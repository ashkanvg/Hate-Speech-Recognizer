@article{FORTUNA2021102524,
title = {How well do hate speech, toxicity, abusive and offensive language classification models generalize across datasets?},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102524},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102524},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000339},
author = {Paula Fortuna and Juan Soler-Company and Leo Wanner},
keywords = {Hate speech, Offensive language, Classification, Generalization},
abstract = {A considerable body of research deals with the automatic identification of hate speech and related phenomena. However, cross-dataset model generalization remains a challenge. In this context, we address two still open central questions: (i) to what extent does the generalization depend on the model and the composition and annotation of the training data in terms of different categories?, and (ii) do specific features of the datasets or models influence the generalization potential? To answer (i), we experiment with BERT, ALBERT, fastText, and SVM models trained on nine common public English datasets, whose class (or category) labels are standardized (and thus made comparable), in intra- and cross-dataset setups. The experiments show that indeed the generalization varies from model to model and that some of the categories (e.g., ‘toxic’, ‘abusive’, or ‘offensive’) serve better as cross-dataset training categories than others (e.g., ‘hate speech’). To answer (ii), we use a Random Forest model for assessing the relevance of different model and dataset features during the prediction of the performance of 450 BERT, 450 ALBERT, 450 fastText, and 348 SVM binary abusive language classifiers (1698 in total). We find that in order to generalize well, a model already needs to perform well in an intra-dataset scenario. Furthermore, we find that some other parameters are equally decisive for the success of the generalization, including, e.g., the training and target categories and the percentage of the out-of-domain vocabulary.}
}


@misc{davidson2017automated,
      title={Automated Hate Speech Detection and the Problem of Offensive Language}, 
      author={Thomas Davidson and Dana Warmsley and Michael Macy and Ingmar Weber},
      year={2017},
      eprint={1703.04009},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{elsafoury2020cyberbullying,
  author = {Fatma Elsafoury},
  title = {Cyberbullying datasets},
  year = {2020},
  howpublished = {\url{https://doi.org/10.17632/jf4pzyvnpj.1}},
  note = {Mendeley Data, V1}
}

@misc{agarwal2024twitter,
  author = {Rahul Agarwal},
  title = {Twitter Hate Speech Data},
  year = {2024},
  howpublished = {\url{https://www.kaggle.com/datasets/vkrahul/twitter-hate-speech/data}},
  note = {Kaggle}
}

@misc{bhargava2021generalization,
      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, 
      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},
      year={2021},
      eprint={2110.01518},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-1908-08962,
  author    = {Iulia Turc and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/1908.08962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08962},
  eprinttype = {arXiv},
  eprint    = {1908.08962},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{jiao2020tinybert,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{rigatti2017random,
  title={Random forest},
  author={Rigatti, Steven J},
  journal={Journal of Insurance Medicine},
  volume={47},
  number={1},
  pages={31--39},
  year={2017},
  publisher={American Academy of Insurance Medicine 1700 Magnavox Way, Fort Wayne, IN 46804}
}

@article{song2015decision,
  title={Decision tree methods: applications for classification and prediction},
  author={Song, Yan-Yan and Ying, LU},
  journal={Shanghai archives of psychiatry},
  volume={27},
  number={2},
  pages={130},
  year={2015},
  publisher={Shanghai Mental Health Center}
}

@article{peterson2009k,
  title={K-nearest neighbor},
  author={Peterson, Leif E},
  journal={Scholarpedia},
  volume={4},
  number={2},
  pages={1883},
  year={2009}
}

@article{lavalley2008logistic,
  title={Logistic regression},
  author={LaValley, Michael P},
  journal={Circulation},
  volume={117},
  number={18},
  pages={2395--2399},
  year={2008},
  publisher={Am Heart Assoc}
}

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusm{\~a}o, Pedro Porto Buarque and others},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}